{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING NECESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('precision',0)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_cases = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n",
    "death_cases = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\"\n",
    "recovered_cases = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df = pd.read_csv(confirmed_cases)\n",
    "death_df = pd.read_csv(death_cases)\n",
    "recovered_df = pd.read_csv(recovered_cases)\n",
    "dates = confirmed_df.drop(['Province/State', 'Country/Region','Lat','Long'], axis = 1).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1/22/20', '1/23/20', '1/24/20', '1/25/20', '1/26/20', '1/27/20',\n",
       "       '1/28/20', '1/29/20', '1/30/20', '1/31/20',\n",
       "       ...\n",
       "       '11/11/21', '11/12/21', '11/13/21', '11/14/21', '11/15/21', '11/16/21',\n",
       "       '11/17/21', '11/18/21', '11/19/21', '11/20/21'],\n",
       "      dtype='object', length=669)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>1/22/20</th>\n",
       "      <th>1/23/20</th>\n",
       "      <th>1/24/20</th>\n",
       "      <th>1/25/20</th>\n",
       "      <th>1/26/20</th>\n",
       "      <th>1/27/20</th>\n",
       "      <th>...</th>\n",
       "      <th>11/11/21</th>\n",
       "      <th>11/12/21</th>\n",
       "      <th>11/13/21</th>\n",
       "      <th>11/14/21</th>\n",
       "      <th>11/15/21</th>\n",
       "      <th>11/16/21</th>\n",
       "      <th>11/17/21</th>\n",
       "      <th>11/18/21</th>\n",
       "      <th>11/19/21</th>\n",
       "      <th>11/20/21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>34</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>156456</td>\n",
       "      <td>156487</td>\n",
       "      <td>156510</td>\n",
       "      <td>156552</td>\n",
       "      <td>156610</td>\n",
       "      <td>156649</td>\n",
       "      <td>156739</td>\n",
       "      <td>156739</td>\n",
       "      <td>156812</td>\n",
       "      <td>156864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Albania</td>\n",
       "      <td>41</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>191440</td>\n",
       "      <td>192013</td>\n",
       "      <td>192600</td>\n",
       "      <td>193075</td>\n",
       "      <td>193269</td>\n",
       "      <td>193856</td>\n",
       "      <td>194472</td>\n",
       "      <td>195021</td>\n",
       "      <td>195523</td>\n",
       "      <td>195988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>207624</td>\n",
       "      <td>207764</td>\n",
       "      <td>207873</td>\n",
       "      <td>207970</td>\n",
       "      <td>208104</td>\n",
       "      <td>208245</td>\n",
       "      <td>208380</td>\n",
       "      <td>208532</td>\n",
       "      <td>208695</td>\n",
       "      <td>208839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>15744</td>\n",
       "      <td>15819</td>\n",
       "      <td>15819</td>\n",
       "      <td>15819</td>\n",
       "      <td>15907</td>\n",
       "      <td>15929</td>\n",
       "      <td>15972</td>\n",
       "      <td>16035</td>\n",
       "      <td>16086</td>\n",
       "      <td>16086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Angola</td>\n",
       "      <td>-11</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>64857</td>\n",
       "      <td>64875</td>\n",
       "      <td>64899</td>\n",
       "      <td>64913</td>\n",
       "      <td>64913</td>\n",
       "      <td>64940</td>\n",
       "      <td>64968</td>\n",
       "      <td>64985</td>\n",
       "      <td>64997</td>\n",
       "      <td>65011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>14</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1000897</td>\n",
       "      <td>1009879</td>\n",
       "      <td>1018346</td>\n",
       "      <td>1026522</td>\n",
       "      <td>1035138</td>\n",
       "      <td>1045397</td>\n",
       "      <td>1055246</td>\n",
       "      <td>1065469</td>\n",
       "      <td>1075094</td>\n",
       "      <td>1084625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>NaN</td>\n",
       "      <td>West Bank and Gaza</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>456407</td>\n",
       "      <td>456632</td>\n",
       "      <td>456632</td>\n",
       "      <td>456632</td>\n",
       "      <td>457154</td>\n",
       "      <td>457390</td>\n",
       "      <td>457477</td>\n",
       "      <td>457729</td>\n",
       "      <td>457950</td>\n",
       "      <td>457950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>16</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9907</td>\n",
       "      <td>9912</td>\n",
       "      <td>9918</td>\n",
       "      <td>9918</td>\n",
       "      <td>9936</td>\n",
       "      <td>9936</td>\n",
       "      <td>9947</td>\n",
       "      <td>9950</td>\n",
       "      <td>9954</td>\n",
       "      <td>9955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Zambia</td>\n",
       "      <td>-13</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>209953</td>\n",
       "      <td>209963</td>\n",
       "      <td>209971</td>\n",
       "      <td>209983</td>\n",
       "      <td>209996</td>\n",
       "      <td>210008</td>\n",
       "      <td>210020</td>\n",
       "      <td>210036</td>\n",
       "      <td>210043</td>\n",
       "      <td>210057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>-19</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>133329</td>\n",
       "      <td>133329</td>\n",
       "      <td>133393</td>\n",
       "      <td>133428</td>\n",
       "      <td>133438</td>\n",
       "      <td>133505</td>\n",
       "      <td>133557</td>\n",
       "      <td>133593</td>\n",
       "      <td>133593</td>\n",
       "      <td>133615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 673 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Province/State      Country/Region  Lat  Long  1/22/20  1/23/20  1/24/20  \\\n",
       "0              NaN         Afghanistan   34    68        0        0        0   \n",
       "1              NaN             Albania   41    20        0        0        0   \n",
       "2              NaN             Algeria   28     2        0        0        0   \n",
       "3              NaN             Andorra   43     2        0        0        0   \n",
       "4              NaN              Angola  -11    18        0        0        0   \n",
       "..             ...                 ...  ...   ...      ...      ...      ...   \n",
       "275            NaN             Vietnam   14   108        0        2        2   \n",
       "276            NaN  West Bank and Gaza   32    35        0        0        0   \n",
       "277            NaN               Yemen   16    49        0        0        0   \n",
       "278            NaN              Zambia  -13    28        0        0        0   \n",
       "279            NaN            Zimbabwe  -19    29        0        0        0   \n",
       "\n",
       "     1/25/20  1/26/20  1/27/20  ...  11/11/21  11/12/21  11/13/21  11/14/21  \\\n",
       "0          0        0        0  ...    156456    156487    156510    156552   \n",
       "1          0        0        0  ...    191440    192013    192600    193075   \n",
       "2          0        0        0  ...    207624    207764    207873    207970   \n",
       "3          0        0        0  ...     15744     15819     15819     15819   \n",
       "4          0        0        0  ...     64857     64875     64899     64913   \n",
       "..       ...      ...      ...  ...       ...       ...       ...       ...   \n",
       "275        2        2        2  ...   1000897   1009879   1018346   1026522   \n",
       "276        0        0        0  ...    456407    456632    456632    456632   \n",
       "277        0        0        0  ...      9907      9912      9918      9918   \n",
       "278        0        0        0  ...    209953    209963    209971    209983   \n",
       "279        0        0        0  ...    133329    133329    133393    133428   \n",
       "\n",
       "     11/15/21  11/16/21  11/17/21  11/18/21  11/19/21  11/20/21  \n",
       "0      156610    156649    156739    156739    156812    156864  \n",
       "1      193269    193856    194472    195021    195523    195988  \n",
       "2      208104    208245    208380    208532    208695    208839  \n",
       "3       15907     15929     15972     16035     16086     16086  \n",
       "4       64913     64940     64968     64985     64997     65011  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "275   1035138   1045397   1055246   1065469   1075094   1084625  \n",
       "276    457154    457390    457477    457729    457950    457950  \n",
       "277      9936      9936      9947      9950      9954      9955  \n",
       "278    209996    210008    210020    210036    210043    210057  \n",
       "279    133438    133505    133557    133593    133593    133615  \n",
       "\n",
       "[280 rows x 673 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confirmed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1/22/20</th>\n",
       "      <th>1/23/20</th>\n",
       "      <th>1/24/20</th>\n",
       "      <th>1/25/20</th>\n",
       "      <th>1/26/20</th>\n",
       "      <th>1/27/20</th>\n",
       "      <th>1/28/20</th>\n",
       "      <th>1/29/20</th>\n",
       "      <th>1/30/20</th>\n",
       "      <th>1/31/20</th>\n",
       "      <th>...</th>\n",
       "      <th>11/11/21</th>\n",
       "      <th>11/12/21</th>\n",
       "      <th>11/13/21</th>\n",
       "      <th>11/14/21</th>\n",
       "      <th>11/15/21</th>\n",
       "      <th>11/16/21</th>\n",
       "      <th>11/17/21</th>\n",
       "      <th>11/18/21</th>\n",
       "      <th>11/19/21</th>\n",
       "      <th>11/20/21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>34414186</td>\n",
       "      <td>34426036</td>\n",
       "      <td>34437307</td>\n",
       "      <td>34447536</td>\n",
       "      <td>34456401</td>\n",
       "      <td>34466598</td>\n",
       "      <td>34478517</td>\n",
       "      <td>34489623</td>\n",
       "      <td>34499925</td>\n",
       "      <td>34510413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 669 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1/22/20  1/23/20  1/24/20  1/25/20  1/26/20  1/27/20  1/28/20  1/29/20  \\\n",
       "147        0        0        0        0        0        0        0        0   \n",
       "\n",
       "     1/30/20  1/31/20  ...  11/11/21  11/12/21  11/13/21  11/14/21  11/15/21  \\\n",
       "147        1        1  ...  34414186  34426036  34437307  34447536  34456401   \n",
       "\n",
       "     11/16/21  11/17/21  11/18/21  11/19/21  11/20/21  \n",
       "147  34466598  34478517  34489623  34499925  34510413  \n",
       "\n",
       "[1 rows x 669 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "india_confirmed = confirmed_df[confirmed_df['Country/Region']=='India']\n",
    "india_confirmed = india_confirmed.drop(['Province/State', 'Country/Region','Lat','Long'], axis = 1)\n",
    "india_confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confirmed</th>\n",
       "      <th>recovered</th>\n",
       "      <th>death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1/24/20</th>\n",
       "      <td>941</td>\n",
       "      <td>39</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/25/20</th>\n",
       "      <td>1434</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/26/20</th>\n",
       "      <td>2118</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/27/20</th>\n",
       "      <td>2927</td>\n",
       "      <td>65</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/28/20</th>\n",
       "      <td>5578</td>\n",
       "      <td>108</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/16/21</th>\n",
       "      <td>254835174</td>\n",
       "      <td>0</td>\n",
       "      <td>5114874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/17/21</th>\n",
       "      <td>255460040</td>\n",
       "      <td>0</td>\n",
       "      <td>5124002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/18/21</th>\n",
       "      <td>256072650</td>\n",
       "      <td>0</td>\n",
       "      <td>5132202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/19/21</th>\n",
       "      <td>256692023</td>\n",
       "      <td>0</td>\n",
       "      <td>5140519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/20/21</th>\n",
       "      <td>257168692</td>\n",
       "      <td>0</td>\n",
       "      <td>5146467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>667 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          confirmed  recovered    death\n",
       "1/24/20         941         39       26\n",
       "1/25/20        1434         42       42\n",
       "1/26/20        2118         56       56\n",
       "1/27/20        2927         65       82\n",
       "1/28/20        5578        108      131\n",
       "...             ...        ...      ...\n",
       "11/16/21  254835174          0  5114874\n",
       "11/17/21  255460040          0  5124002\n",
       "11/18/21  256072650          0  5132202\n",
       "11/19/21  256692023          0  5140519\n",
       "11/20/21  257168692          0  5146467\n",
       "\n",
       "[667 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confirmed = confirmed_df.drop(['Province/State', 'Country/Region','Lat','Long'], axis = 1)\n",
    "confirmed = confirmed.sum(axis=0,skipna = True)\n",
    "confirmed = pd.DataFrame(confirmed)\n",
    "confirmed = confirmed.rename(columns={0: 'confirmed', \"\" : 'Date'})\n",
    "\n",
    "death = death_df.drop(['Province/State', 'Country/Region','Lat','Long'], axis = 1)\n",
    "death = death.sum(axis=0,skipna = True)\n",
    "death = pd.DataFrame(death)\n",
    "death = death.rename(columns={0: 'death', \"\" : 'Date'})\n",
    "\n",
    "recovered = recovered_df.drop(['Province/State', 'Country/Region','Lat','Long'], axis = 1)\n",
    "recovered = recovered.sum(axis=0,skipna = True)\n",
    "recovered = pd.DataFrame(recovered)\n",
    "recovered = recovered.rename(columns={0: 'recovered', \"\" : 'Date'})\n",
    "\n",
    "lstm_dataset = confirmed\n",
    "\n",
    "lstm_dataset['recovered'] = recovered['recovered']\n",
    "lstm_dataset['death'] = death['death']\n",
    "lstm_dataset.tail(667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(669, 3)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = lstm_dataset.iloc[:,0:1].values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scale = MinMaxScaler(feature_range=(0,1))\n",
    "training_set_scaled = scale.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x, y = [], []\n",
    "time_steps = 45\n",
    "for i in range(len(training_set) - time_steps):\n",
    "    x_ = training_set_scaled[i:(i+time_steps), 0]\n",
    "    x.append(x_)\n",
    "    y.append(training_set_scaled[i+time_steps, 0])\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PARTITIONING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(x) * 0.8)\n",
    "x_train = x[:split]\n",
    "x_test = x[split:]\n",
    "y_train = y[:split]\n",
    "y_test = y[split:]\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 1, 48)             18048     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1, 48)             0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 1, 48)             18624     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1, 48)             0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 48)                18624     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 49        \n",
      "=================================================================\n",
      "Total params: 55,345\n",
      "Trainable params: 55,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(1, time_steps)))\n",
    "model.add(LSTM(48, return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(48, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(48))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(loss = 'mean_squared_error',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['mean_squared_error'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 14s 1s/step - loss: 0.0559 - mean_squared_error: 0.0559 - val_loss: 0.3939 - val_mean_squared_error: 0.3939\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0502 - mean_squared_error: 0.0502 - val_loss: 0.3652 - val_mean_squared_error: 0.3652\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0445 - mean_squared_error: 0.0445 - val_loss: 0.3324 - val_mean_squared_error: 0.3324\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0386 - mean_squared_error: 0.0386 - val_loss: 0.2940 - val_mean_squared_error: 0.2940\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.2490 - val_mean_squared_error: 0.2490\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.1971 - val_mean_squared_error: 0.1971\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.1400 - val_mean_squared_error: 0.1400\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0114 - mean_squared_error: 0.0114 - val_loss: 0.0826 - val_mean_squared_error: 0.0826\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0342 - val_mean_squared_error: 0.0342\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0054 - mean_squared_error: 0.0054 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 1.2120e-04 - val_mean_squared_error: 1.2120e-04\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0071 - mean_squared_error: 0.0071 - val_loss: 2.3325e-04 - val_mean_squared_error: 2.3325e-04\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0068 - val_mean_squared_error: 0.0068\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 75ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 133ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 61ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 1e-10.\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 60ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 1s 207ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 93ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 62ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 61ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 60ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "batchsize = 100\n",
    "epochs =  200\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_mean_squared_error', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=1e-10)\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    batch_size=batchsize,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    shuffle=False,\n",
    "                    callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 3s 5ms/step - loss: 0.0022 - mean_squared_error: 0.0022\n",
      "Performance : 0.22%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_train,y_train,verbose=1)\n",
    "print(\"Performance : %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x224cec2b670>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = scale.inverse_transform(y_pred)\n",
    "y_test = scale.inverse_transform(y_test.reshape(-1,1))\n",
    "plt.plot(y_pred, color='red')\n",
    "plt.plot(y_test, color='blue')\n",
    "plt.title('Actual vs. Predicted Covid Cases (Test Data)')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.xlabel('Day')\n",
    "plt.legend(['predicted', 'actual'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1/22/20', '1/23/20', '1/24/20', '1/25/20', '1/26/20', '1/27/20',\n",
       "       '1/28/20', '1/29/20', '1/30/20', '1/31/20',\n",
       "       ...\n",
       "       '11/11/21', '11/12/21', '11/13/21', '11/14/21', '11/15/21', '11/16/21',\n",
       "       '11/17/21', '11/18/21', '11/19/21', '11/20/21'],\n",
       "      dtype='object', length=669)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0],\n",
       "       [  1],\n",
       "       [  2],\n",
       "       [  3],\n",
       "       [  4],\n",
       "       [  5],\n",
       "       [  6],\n",
       "       [  7],\n",
       "       [  8],\n",
       "       [  9],\n",
       "       [ 10],\n",
       "       [ 11],\n",
       "       [ 12],\n",
       "       [ 13],\n",
       "       [ 14],\n",
       "       [ 15],\n",
       "       [ 16],\n",
       "       [ 17],\n",
       "       [ 18],\n",
       "       [ 19],\n",
       "       [ 20],\n",
       "       [ 21],\n",
       "       [ 22],\n",
       "       [ 23],\n",
       "       [ 24],\n",
       "       [ 25],\n",
       "       [ 26],\n",
       "       [ 27],\n",
       "       [ 28],\n",
       "       [ 29],\n",
       "       [ 30],\n",
       "       [ 31],\n",
       "       [ 32],\n",
       "       [ 33],\n",
       "       [ 34],\n",
       "       [ 35],\n",
       "       [ 36],\n",
       "       [ 37],\n",
       "       [ 38],\n",
       "       [ 39],\n",
       "       [ 40],\n",
       "       [ 41],\n",
       "       [ 42],\n",
       "       [ 43],\n",
       "       [ 44],\n",
       "       [ 45],\n",
       "       [ 46],\n",
       "       [ 47],\n",
       "       [ 48],\n",
       "       [ 49],\n",
       "       [ 50],\n",
       "       [ 51],\n",
       "       [ 52],\n",
       "       [ 53],\n",
       "       [ 54],\n",
       "       [ 55],\n",
       "       [ 56],\n",
       "       [ 57],\n",
       "       [ 58],\n",
       "       [ 59],\n",
       "       [ 60],\n",
       "       [ 61],\n",
       "       [ 62],\n",
       "       [ 63],\n",
       "       [ 64],\n",
       "       [ 65],\n",
       "       [ 66],\n",
       "       [ 67],\n",
       "       [ 68],\n",
       "       [ 69],\n",
       "       [ 70],\n",
       "       [ 71],\n",
       "       [ 72],\n",
       "       [ 73],\n",
       "       [ 74],\n",
       "       [ 75],\n",
       "       [ 76],\n",
       "       [ 77],\n",
       "       [ 78],\n",
       "       [ 79],\n",
       "       [ 80],\n",
       "       [ 81],\n",
       "       [ 82],\n",
       "       [ 83],\n",
       "       [ 84],\n",
       "       [ 85],\n",
       "       [ 86],\n",
       "       [ 87],\n",
       "       [ 88],\n",
       "       [ 89],\n",
       "       [ 90],\n",
       "       [ 91],\n",
       "       [ 92],\n",
       "       [ 93],\n",
       "       [ 94],\n",
       "       [ 95],\n",
       "       [ 96],\n",
       "       [ 97],\n",
       "       [ 98],\n",
       "       [ 99],\n",
       "       [100],\n",
       "       [101],\n",
       "       [102],\n",
       "       [103],\n",
       "       [104],\n",
       "       [105],\n",
       "       [106],\n",
       "       [107],\n",
       "       [108],\n",
       "       [109],\n",
       "       [110],\n",
       "       [111],\n",
       "       [112],\n",
       "       [113],\n",
       "       [114],\n",
       "       [115],\n",
       "       [116],\n",
       "       [117],\n",
       "       [118],\n",
       "       [119],\n",
       "       [120],\n",
       "       [121],\n",
       "       [122],\n",
       "       [123],\n",
       "       [124],\n",
       "       [125],\n",
       "       [126],\n",
       "       [127],\n",
       "       [128],\n",
       "       [129],\n",
       "       [130],\n",
       "       [131],\n",
       "       [132],\n",
       "       [133],\n",
       "       [134],\n",
       "       [135],\n",
       "       [136],\n",
       "       [137],\n",
       "       [138],\n",
       "       [139],\n",
       "       [140],\n",
       "       [141],\n",
       "       [142],\n",
       "       [143],\n",
       "       [144],\n",
       "       [145],\n",
       "       [146],\n",
       "       [147],\n",
       "       [148],\n",
       "       [149],\n",
       "       [150],\n",
       "       [151],\n",
       "       [152],\n",
       "       [153],\n",
       "       [154],\n",
       "       [155],\n",
       "       [156],\n",
       "       [157],\n",
       "       [158],\n",
       "       [159],\n",
       "       [160],\n",
       "       [161],\n",
       "       [162],\n",
       "       [163],\n",
       "       [164],\n",
       "       [165],\n",
       "       [166],\n",
       "       [167],\n",
       "       [168],\n",
       "       [169],\n",
       "       [170],\n",
       "       [171],\n",
       "       [172],\n",
       "       [173],\n",
       "       [174],\n",
       "       [175],\n",
       "       [176],\n",
       "       [177],\n",
       "       [178],\n",
       "       [179],\n",
       "       [180],\n",
       "       [181],\n",
       "       [182],\n",
       "       [183],\n",
       "       [184],\n",
       "       [185],\n",
       "       [186],\n",
       "       [187],\n",
       "       [188],\n",
       "       [189],\n",
       "       [190],\n",
       "       [191],\n",
       "       [192],\n",
       "       [193],\n",
       "       [194],\n",
       "       [195],\n",
       "       [196],\n",
       "       [197],\n",
       "       [198],\n",
       "       [199],\n",
       "       [200],\n",
       "       [201],\n",
       "       [202],\n",
       "       [203],\n",
       "       [204],\n",
       "       [205],\n",
       "       [206],\n",
       "       [207],\n",
       "       [208],\n",
       "       [209],\n",
       "       [210],\n",
       "       [211],\n",
       "       [212],\n",
       "       [213],\n",
       "       [214],\n",
       "       [215],\n",
       "       [216],\n",
       "       [217],\n",
       "       [218],\n",
       "       [219],\n",
       "       [220],\n",
       "       [221],\n",
       "       [222],\n",
       "       [223],\n",
       "       [224],\n",
       "       [225],\n",
       "       [226],\n",
       "       [227],\n",
       "       [228],\n",
       "       [229],\n",
       "       [230],\n",
       "       [231],\n",
       "       [232],\n",
       "       [233],\n",
       "       [234],\n",
       "       [235],\n",
       "       [236],\n",
       "       [237],\n",
       "       [238],\n",
       "       [239],\n",
       "       [240],\n",
       "       [241],\n",
       "       [242],\n",
       "       [243],\n",
       "       [244],\n",
       "       [245],\n",
       "       [246],\n",
       "       [247],\n",
       "       [248],\n",
       "       [249],\n",
       "       [250],\n",
       "       [251],\n",
       "       [252],\n",
       "       [253],\n",
       "       [254],\n",
       "       [255],\n",
       "       [256],\n",
       "       [257],\n",
       "       [258],\n",
       "       [259],\n",
       "       [260],\n",
       "       [261],\n",
       "       [262],\n",
       "       [263],\n",
       "       [264],\n",
       "       [265],\n",
       "       [266],\n",
       "       [267],\n",
       "       [268],\n",
       "       [269],\n",
       "       [270],\n",
       "       [271],\n",
       "       [272],\n",
       "       [273],\n",
       "       [274],\n",
       "       [275],\n",
       "       [276],\n",
       "       [277],\n",
       "       [278],\n",
       "       [279],\n",
       "       [280],\n",
       "       [281],\n",
       "       [282],\n",
       "       [283],\n",
       "       [284],\n",
       "       [285],\n",
       "       [286],\n",
       "       [287],\n",
       "       [288],\n",
       "       [289],\n",
       "       [290],\n",
       "       [291],\n",
       "       [292],\n",
       "       [293],\n",
       "       [294],\n",
       "       [295],\n",
       "       [296],\n",
       "       [297],\n",
       "       [298],\n",
       "       [299],\n",
       "       [300],\n",
       "       [301],\n",
       "       [302],\n",
       "       [303],\n",
       "       [304],\n",
       "       [305],\n",
       "       [306],\n",
       "       [307],\n",
       "       [308],\n",
       "       [309],\n",
       "       [310],\n",
       "       [311],\n",
       "       [312],\n",
       "       [313],\n",
       "       [314],\n",
       "       [315],\n",
       "       [316],\n",
       "       [317],\n",
       "       [318],\n",
       "       [319],\n",
       "       [320],\n",
       "       [321],\n",
       "       [322],\n",
       "       [323],\n",
       "       [324],\n",
       "       [325],\n",
       "       [326],\n",
       "       [327],\n",
       "       [328],\n",
       "       [329],\n",
       "       [330],\n",
       "       [331],\n",
       "       [332],\n",
       "       [333],\n",
       "       [334],\n",
       "       [335],\n",
       "       [336],\n",
       "       [337],\n",
       "       [338],\n",
       "       [339],\n",
       "       [340],\n",
       "       [341],\n",
       "       [342],\n",
       "       [343],\n",
       "       [344],\n",
       "       [345],\n",
       "       [346],\n",
       "       [347],\n",
       "       [348],\n",
       "       [349],\n",
       "       [350],\n",
       "       [351],\n",
       "       [352],\n",
       "       [353],\n",
       "       [354],\n",
       "       [355],\n",
       "       [356],\n",
       "       [357],\n",
       "       [358],\n",
       "       [359],\n",
       "       [360],\n",
       "       [361],\n",
       "       [362],\n",
       "       [363],\n",
       "       [364],\n",
       "       [365],\n",
       "       [366],\n",
       "       [367],\n",
       "       [368],\n",
       "       [369],\n",
       "       [370],\n",
       "       [371],\n",
       "       [372],\n",
       "       [373],\n",
       "       [374],\n",
       "       [375],\n",
       "       [376],\n",
       "       [377],\n",
       "       [378],\n",
       "       [379],\n",
       "       [380],\n",
       "       [381],\n",
       "       [382],\n",
       "       [383],\n",
       "       [384],\n",
       "       [385],\n",
       "       [386],\n",
       "       [387],\n",
       "       [388],\n",
       "       [389],\n",
       "       [390],\n",
       "       [391],\n",
       "       [392],\n",
       "       [393],\n",
       "       [394],\n",
       "       [395],\n",
       "       [396],\n",
       "       [397],\n",
       "       [398],\n",
       "       [399],\n",
       "       [400],\n",
       "       [401],\n",
       "       [402],\n",
       "       [403],\n",
       "       [404],\n",
       "       [405],\n",
       "       [406],\n",
       "       [407],\n",
       "       [408],\n",
       "       [409],\n",
       "       [410],\n",
       "       [411],\n",
       "       [412],\n",
       "       [413],\n",
       "       [414],\n",
       "       [415],\n",
       "       [416],\n",
       "       [417],\n",
       "       [418],\n",
       "       [419],\n",
       "       [420],\n",
       "       [421],\n",
       "       [422],\n",
       "       [423],\n",
       "       [424],\n",
       "       [425],\n",
       "       [426],\n",
       "       [427],\n",
       "       [428],\n",
       "       [429],\n",
       "       [430],\n",
       "       [431],\n",
       "       [432],\n",
       "       [433],\n",
       "       [434],\n",
       "       [435],\n",
       "       [436],\n",
       "       [437],\n",
       "       [438],\n",
       "       [439],\n",
       "       [440],\n",
       "       [441],\n",
       "       [442],\n",
       "       [443],\n",
       "       [444],\n",
       "       [445],\n",
       "       [446],\n",
       "       [447],\n",
       "       [448],\n",
       "       [449],\n",
       "       [450],\n",
       "       [451],\n",
       "       [452],\n",
       "       [453],\n",
       "       [454],\n",
       "       [455],\n",
       "       [456],\n",
       "       [457],\n",
       "       [458],\n",
       "       [459],\n",
       "       [460],\n",
       "       [461],\n",
       "       [462],\n",
       "       [463],\n",
       "       [464],\n",
       "       [465],\n",
       "       [466],\n",
       "       [467],\n",
       "       [468],\n",
       "       [469],\n",
       "       [470],\n",
       "       [471],\n",
       "       [472],\n",
       "       [473],\n",
       "       [474],\n",
       "       [475],\n",
       "       [476],\n",
       "       [477],\n",
       "       [478],\n",
       "       [479],\n",
       "       [480],\n",
       "       [481],\n",
       "       [482],\n",
       "       [483],\n",
       "       [484],\n",
       "       [485],\n",
       "       [486],\n",
       "       [487],\n",
       "       [488],\n",
       "       [489],\n",
       "       [490],\n",
       "       [491],\n",
       "       [492],\n",
       "       [493],\n",
       "       [494],\n",
       "       [495],\n",
       "       [496],\n",
       "       [497],\n",
       "       [498],\n",
       "       [499],\n",
       "       [500],\n",
       "       [501],\n",
       "       [502],\n",
       "       [503],\n",
       "       [504],\n",
       "       [505],\n",
       "       [506],\n",
       "       [507],\n",
       "       [508],\n",
       "       [509],\n",
       "       [510],\n",
       "       [511],\n",
       "       [512],\n",
       "       [513],\n",
       "       [514],\n",
       "       [515],\n",
       "       [516],\n",
       "       [517],\n",
       "       [518],\n",
       "       [519],\n",
       "       [520],\n",
       "       [521],\n",
       "       [522],\n",
       "       [523],\n",
       "       [524],\n",
       "       [525],\n",
       "       [526],\n",
       "       [527],\n",
       "       [528],\n",
       "       [529],\n",
       "       [530],\n",
       "       [531],\n",
       "       [532],\n",
       "       [533],\n",
       "       [534],\n",
       "       [535],\n",
       "       [536],\n",
       "       [537],\n",
       "       [538],\n",
       "       [539],\n",
       "       [540],\n",
       "       [541],\n",
       "       [542],\n",
       "       [543],\n",
       "       [544],\n",
       "       [545],\n",
       "       [546],\n",
       "       [547],\n",
       "       [548],\n",
       "       [549],\n",
       "       [550],\n",
       "       [551],\n",
       "       [552],\n",
       "       [553],\n",
       "       [554],\n",
       "       [555],\n",
       "       [556],\n",
       "       [557],\n",
       "       [558],\n",
       "       [559],\n",
       "       [560],\n",
       "       [561],\n",
       "       [562],\n",
       "       [563],\n",
       "       [564],\n",
       "       [565],\n",
       "       [566],\n",
       "       [567],\n",
       "       [568],\n",
       "       [569],\n",
       "       [570],\n",
       "       [571],\n",
       "       [572],\n",
       "       [573],\n",
       "       [574],\n",
       "       [575],\n",
       "       [576],\n",
       "       [577],\n",
       "       [578],\n",
       "       [579],\n",
       "       [580],\n",
       "       [581],\n",
       "       [582],\n",
       "       [583],\n",
       "       [584],\n",
       "       [585],\n",
       "       [586],\n",
       "       [587],\n",
       "       [588],\n",
       "       [589],\n",
       "       [590],\n",
       "       [591],\n",
       "       [592],\n",
       "       [593],\n",
       "       [594],\n",
       "       [595],\n",
       "       [596],\n",
       "       [597],\n",
       "       [598],\n",
       "       [599],\n",
       "       [600],\n",
       "       [601],\n",
       "       [602],\n",
       "       [603],\n",
       "       [604],\n",
       "       [605],\n",
       "       [606],\n",
       "       [607],\n",
       "       [608],\n",
       "       [609],\n",
       "       [610],\n",
       "       [611],\n",
       "       [612],\n",
       "       [613],\n",
       "       [614],\n",
       "       [615],\n",
       "       [616],\n",
       "       [617],\n",
       "       [618],\n",
       "       [619],\n",
       "       [620],\n",
       "       [621],\n",
       "       [622],\n",
       "       [623],\n",
       "       [624],\n",
       "       [625],\n",
       "       [626],\n",
       "       [627],\n",
       "       [628],\n",
       "       [629],\n",
       "       [630],\n",
       "       [631],\n",
       "       [632],\n",
       "       [633],\n",
       "       [634],\n",
       "       [635],\n",
       "       [636],\n",
       "       [637],\n",
       "       [638],\n",
       "       [639],\n",
       "       [640],\n",
       "       [641],\n",
       "       [642],\n",
       "       [643],\n",
       "       [644],\n",
       "       [645],\n",
       "       [646],\n",
       "       [647],\n",
       "       [648],\n",
       "       [649],\n",
       "       [650],\n",
       "       [651],\n",
       "       [652],\n",
       "       [653],\n",
       "       [654],\n",
       "       [655],\n",
       "       [656],\n",
       "       [657],\n",
       "       [658],\n",
       "       [659],\n",
       "       [660],\n",
       "       [661],\n",
       "       [662],\n",
       "       [663],\n",
       "       [664],\n",
       "       [665],\n",
       "       [666],\n",
       "       [667],\n",
       "       [668]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_days = 20\n",
    "forecast = np.array([i for i in range(len(dates)+pred_days)]).reshape(-1, 1)\n",
    "adjusted_dates = forecast[:-20]\n",
    "adjusted_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "start = '1/22/2020'\n",
    "start_date = datetime.datetime.strptime(start, '%m/%d/%Y')\n",
    "forecast_dates = []\n",
    "for i in range(len(forecast)):\n",
    "    forecast_dates.append((start_date + datetime.timedelta(days=i)).strftime('%m/%d/%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Covid cases worldwide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/21/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11/22/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11/23/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11/24/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11/25/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11/26/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11/27/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11/28/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11/29/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11/30/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12/01/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12/02/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12/03/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12/04/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12/05/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12/06/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12/07/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12/08/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12/09/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12/10/2021</td>\n",
       "      <td>2e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Covid cases worldwide\n",
       "1   11/21/2021                  2e+08\n",
       "2   11/22/2021                  2e+08\n",
       "3   11/23/2021                  2e+08\n",
       "4   11/24/2021                  2e+08\n",
       "5   11/25/2021                  2e+08\n",
       "6   11/26/2021                  2e+08\n",
       "7   11/27/2021                  2e+08\n",
       "8   11/28/2021                  2e+08\n",
       "9   11/29/2021                  2e+08\n",
       "10  11/30/2021                  2e+08\n",
       "11  12/01/2021                  2e+08\n",
       "12  12/02/2021                  2e+08\n",
       "13  12/03/2021                  2e+08\n",
       "14  12/04/2021                  2e+08\n",
       "15  12/05/2021                  2e+08\n",
       "16  12/06/2021                  2e+08\n",
       "17  12/07/2021                  2e+08\n",
       "18  12/08/2021                  2e+08\n",
       "19  12/09/2021                  2e+08\n",
       "20  12/10/2021                  2e+08"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "start = '1/22/2020'\n",
    "start_date = datetime.datetime.strptime(start, '%m/%d/%Y')\n",
    "forecast_dates = []\n",
    "for i in range(len(forecast)):\n",
    "    forecast_dates.append((start_date + datetime.timedelta(days=i)).strftime('%m/%d/%Y'))\n",
    "y_pred = y_pred.reshape(1,-1)[0]\n",
    "index = []\n",
    "for i in range(1,21):\n",
    "    index.append(i)\n",
    "df = pd.DataFrame({'Date': forecast_dates[-20:], 'Covid cases worldwide': np.round(y_pred[-20:])},index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_confirmed = confirmed_df[confirmed_df['Country/Region']=='India']\n",
    "india_confirmed = india_confirmed.drop(['Province/State', 'Country/Region','Lat','Long'], axis = 1)\n",
    "india_recovered = recovered_df[recovered_df['Country/Region']=='India']\n",
    "india_recovered = india_recovered.drop(['Province/State', 'Country/Region','Lat','Long'], axis = 1)\n",
    "india_death = death_df[confirmed_df['Country/Region']=='India']\n",
    "india_death = india_death.drop(['Province/State', 'Country/Region','Lat','Long'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1/1/21</th>\n",
       "      <td>10305788</td>\n",
       "      <td>9929568</td>\n",
       "      <td>149218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/2/21</th>\n",
       "      <td>10323965</td>\n",
       "      <td>9927310</td>\n",
       "      <td>149435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/3/21</th>\n",
       "      <td>10340469</td>\n",
       "      <td>9946867</td>\n",
       "      <td>149649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/4/21</th>\n",
       "      <td>10356844</td>\n",
       "      <td>9975958</td>\n",
       "      <td>149850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/5/21</th>\n",
       "      <td>10374932</td>\n",
       "      <td>9997272</td>\n",
       "      <td>150114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/16/21</th>\n",
       "      <td>34466598</td>\n",
       "      <td>0</td>\n",
       "      <td>464153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/17/21</th>\n",
       "      <td>34478517</td>\n",
       "      <td>0</td>\n",
       "      <td>464623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/18/21</th>\n",
       "      <td>34489623</td>\n",
       "      <td>0</td>\n",
       "      <td>465082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/19/21</th>\n",
       "      <td>34499925</td>\n",
       "      <td>0</td>\n",
       "      <td>465349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/20/21</th>\n",
       "      <td>34510413</td>\n",
       "      <td>0</td>\n",
       "      <td>465662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Confirmed  Recovered   Death\n",
       "1/1/21     10305788    9929568  149218\n",
       "1/2/21     10323965    9927310  149435\n",
       "1/3/21     10340469    9946867  149649\n",
       "1/4/21     10356844    9975958  149850\n",
       "1/5/21     10374932    9997272  150114\n",
       "...             ...        ...     ...\n",
       "11/16/21   34466598          0  464153\n",
       "11/17/21   34478517          0  464623\n",
       "11/18/21   34489623          0  465082\n",
       "11/19/21   34499925          0  465349\n",
       "11/20/21   34510413          0  465662\n",
       "\n",
       "[324 rows x 3 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "india_confirmed = india_confirmed.transpose().rename(columns={147: 'Confirmed'})\n",
    "india_recovered = india_recovered.transpose().rename(columns={132: 'Recovered'})\n",
    "india_death = india_death.transpose().rename(columns={147: 'Death'})\n",
    "lstm_data = india_confirmed\n",
    "lstm_data['Recovered'] = india_recovered['Recovered']\n",
    "lstm_data['Death'] = india_death['Death']\n",
    "lstm_data = lstm_data[345:]\n",
    "lstm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = lstm_data.iloc[:,0:1].values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scale = MinMaxScaler(feature_range=(0,1))\n",
    "training_set_scaled = scale.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x, y = [], []\n",
    "time_steps = 45\n",
    "for i in range(len(training_set) - time_steps):\n",
    "    x_ = training_set_scaled[i:(i+time_steps), 0]\n",
    "    x.append(x_)\n",
    "    y.append(training_set_scaled[i+time_steps, 0])\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(x) * 0.8)\n",
    "x_train = x[:split]\n",
    "x_test = x[split:]\n",
    "y_train = y[:split]\n",
    "y_test = y[split:]\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 1, 48)             18048     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1, 48)             0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 1, 48)             18624     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1, 48)             0         \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 48)                18624     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 49        \n",
      "=================================================================\n",
      "Total params: 55,345\n",
      "Trainable params: 55,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(1, time_steps)))\n",
    "model.add(LSTM(48, return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(48, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(48))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(loss = 'mean_squared_error',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['mean_squared_error'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 26s 4s/step - loss: 0.3501 - mean_squared_error: 0.3501 - val_loss: 0.8518 - val_mean_squared_error: 0.8518\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 208ms/step - loss: 0.3415 - mean_squared_error: 0.3415 - val_loss: 0.8324 - val_mean_squared_error: 0.8324\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3320 - mean_squared_error: 0.3320 - val_loss: 0.8112 - val_mean_squared_error: 0.8112\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3237 - mean_squared_error: 0.3237 - val_loss: 0.7875 - val_mean_squared_error: 0.7875\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.3122 - mean_squared_error: 0.3122 - val_loss: 0.7606 - val_mean_squared_error: 0.7606\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3005 - mean_squared_error: 0.3005 - val_loss: 0.7296 - val_mean_squared_error: 0.7296\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 250ms/step - loss: 0.2888 - mean_squared_error: 0.2888 - val_loss: 0.6936 - val_mean_squared_error: 0.6936\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.2728 - mean_squared_error: 0.2728 - val_loss: 0.6514 - val_mean_squared_error: 0.6514\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 207ms/step - loss: 0.2537 - mean_squared_error: 0.2537 - val_loss: 0.6020 - val_mean_squared_error: 0.6020\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.2347 - mean_squared_error: 0.2347 - val_loss: 0.5446 - val_mean_squared_error: 0.5446\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 1s 635ms/step - loss: 0.2130 - mean_squared_error: 0.2130 - val_loss: 0.4784 - val_mean_squared_error: 0.4784\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1822 - mean_squared_error: 0.1822 - val_loss: 0.4033 - val_mean_squared_error: 0.4033\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.1577 - mean_squared_error: 0.1577 - val_loss: 0.3206 - val_mean_squared_error: 0.3206\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.1261 - mean_squared_error: 0.1261 - val_loss: 0.2331 - val_mean_squared_error: 0.2331\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0934 - mean_squared_error: 0.0934 - val_loss: 0.1466 - val_mean_squared_error: 0.1466\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.0563 - mean_squared_error: 0.0563 - val_loss: 0.0703 - val_mean_squared_error: 0.0703\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 1s 594ms/step - loss: 0.0344 - mean_squared_error: 0.0344 - val_loss: 0.0171 - val_mean_squared_error: 0.0171\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.0221 - mean_squared_error: 0.0221 - val_loss: 2.3321e-04 - val_mean_squared_error: 2.3321e-04\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0213 - val_mean_squared_error: 0.0213\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 272ms/step - loss: 0.0256 - mean_squared_error: 0.0256 - val_loss: 0.0559 - val_mean_squared_error: 0.0559\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 0.0387 - mean_squared_error: 0.0387 - val_loss: 0.0679 - val_mean_squared_error: 0.0679\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0392 - mean_squared_error: 0.0392 - val_loss: 0.0592 - val_mean_squared_error: 0.0592\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0354 - mean_squared_error: 0.0354 - val_loss: 0.0437 - val_mean_squared_error: 0.0437\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0280 - val_mean_squared_error: 0.0280\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 1s 487ms/step - loss: 0.0297 - mean_squared_error: 0.0297 - val_loss: 0.0208 - val_mean_squared_error: 0.0208\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0144 - val_mean_squared_error: 0.0144\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 275ms/step - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 250ms/step - loss: 0.0163 - mean_squared_error: 0.0163 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0135 - mean_squared_error: 0.0135 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0153 - mean_squared_error: 0.0153 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.0159 - mean_squared_error: 0.0159 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0154 - mean_squared_error: 0.0154 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0176 - mean_squared_error: 0.0176 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0176 - mean_squared_error: 0.0176 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 305ms/step - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 272ms/step - loss: 0.0157 - mean_squared_error: 0.0157 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 413ms/step - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 196ms/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 259ms/step - loss: 0.0144 - mean_squared_error: 0.0144 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.0147 - mean_squared_error: 0.0147 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 280ms/step - loss: 0.0157 - mean_squared_error: 0.0157 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0155 - mean_squared_error: 0.0155 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 202ms/step - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.0158 - mean_squared_error: 0.0158 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 0.0146 - mean_squared_error: 0.0146 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 0.0153 - mean_squared_error: 0.0153 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.0151 - mean_squared_error: 0.0151 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 208ms/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.0156 - mean_squared_error: 0.0156 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.0154 - mean_squared_error: 0.0154 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 248ms/step - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1e-10.\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.0159 - mean_squared_error: 0.0159 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0149 - mean_squared_error: 0.0149 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0152 - mean_squared_error: 0.0152 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 0.0152 - mean_squared_error: 0.0152 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "batchsize = 100\n",
    "epochs =  100\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_mean_squared_error', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=1e-10)\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    batch_size=batchsize,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    shuffle=False,\n",
    "                    callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Performance : 0.71%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_train,y_train,verbose=1)\n",
    "scores\n",
    "print(\"Performance : %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x224e5664ca0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = scale.inverse_transform(y_pred)\n",
    "y_test = scale.inverse_transform(y_test.reshape(-1,1))\n",
    "plt.plot(y_pred, color='red')\n",
    "plt.plot(y_test, color='blue')\n",
    "plt.title('Actual vs. Predicted Covid Cases (India)')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.xlabel('Day')\n",
    "plt.legend(['predicted', 'actual'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0],\n",
       "       [  1],\n",
       "       [  2],\n",
       "       [  3],\n",
       "       [  4],\n",
       "       [  5],\n",
       "       [  6],\n",
       "       [  7],\n",
       "       [  8],\n",
       "       [  9],\n",
       "       [ 10],\n",
       "       [ 11],\n",
       "       [ 12],\n",
       "       [ 13],\n",
       "       [ 14],\n",
       "       [ 15],\n",
       "       [ 16],\n",
       "       [ 17],\n",
       "       [ 18],\n",
       "       [ 19],\n",
       "       [ 20],\n",
       "       [ 21],\n",
       "       [ 22],\n",
       "       [ 23],\n",
       "       [ 24],\n",
       "       [ 25],\n",
       "       [ 26],\n",
       "       [ 27],\n",
       "       [ 28],\n",
       "       [ 29],\n",
       "       [ 30],\n",
       "       [ 31],\n",
       "       [ 32],\n",
       "       [ 33],\n",
       "       [ 34],\n",
       "       [ 35],\n",
       "       [ 36],\n",
       "       [ 37],\n",
       "       [ 38],\n",
       "       [ 39],\n",
       "       [ 40],\n",
       "       [ 41],\n",
       "       [ 42],\n",
       "       [ 43],\n",
       "       [ 44],\n",
       "       [ 45],\n",
       "       [ 46],\n",
       "       [ 47],\n",
       "       [ 48],\n",
       "       [ 49],\n",
       "       [ 50],\n",
       "       [ 51],\n",
       "       [ 52],\n",
       "       [ 53],\n",
       "       [ 54],\n",
       "       [ 55],\n",
       "       [ 56],\n",
       "       [ 57],\n",
       "       [ 58],\n",
       "       [ 59],\n",
       "       [ 60],\n",
       "       [ 61],\n",
       "       [ 62],\n",
       "       [ 63],\n",
       "       [ 64],\n",
       "       [ 65],\n",
       "       [ 66],\n",
       "       [ 67],\n",
       "       [ 68],\n",
       "       [ 69],\n",
       "       [ 70],\n",
       "       [ 71],\n",
       "       [ 72],\n",
       "       [ 73],\n",
       "       [ 74],\n",
       "       [ 75],\n",
       "       [ 76],\n",
       "       [ 77],\n",
       "       [ 78],\n",
       "       [ 79],\n",
       "       [ 80],\n",
       "       [ 81],\n",
       "       [ 82],\n",
       "       [ 83],\n",
       "       [ 84],\n",
       "       [ 85],\n",
       "       [ 86],\n",
       "       [ 87],\n",
       "       [ 88],\n",
       "       [ 89],\n",
       "       [ 90],\n",
       "       [ 91],\n",
       "       [ 92],\n",
       "       [ 93],\n",
       "       [ 94],\n",
       "       [ 95],\n",
       "       [ 96],\n",
       "       [ 97],\n",
       "       [ 98],\n",
       "       [ 99],\n",
       "       [100],\n",
       "       [101],\n",
       "       [102],\n",
       "       [103],\n",
       "       [104],\n",
       "       [105],\n",
       "       [106],\n",
       "       [107],\n",
       "       [108],\n",
       "       [109],\n",
       "       [110],\n",
       "       [111],\n",
       "       [112],\n",
       "       [113],\n",
       "       [114],\n",
       "       [115],\n",
       "       [116],\n",
       "       [117],\n",
       "       [118],\n",
       "       [119],\n",
       "       [120],\n",
       "       [121],\n",
       "       [122],\n",
       "       [123],\n",
       "       [124],\n",
       "       [125],\n",
       "       [126],\n",
       "       [127],\n",
       "       [128],\n",
       "       [129],\n",
       "       [130],\n",
       "       [131],\n",
       "       [132],\n",
       "       [133],\n",
       "       [134],\n",
       "       [135],\n",
       "       [136],\n",
       "       [137],\n",
       "       [138],\n",
       "       [139],\n",
       "       [140],\n",
       "       [141],\n",
       "       [142],\n",
       "       [143],\n",
       "       [144],\n",
       "       [145],\n",
       "       [146],\n",
       "       [147],\n",
       "       [148],\n",
       "       [149],\n",
       "       [150],\n",
       "       [151],\n",
       "       [152],\n",
       "       [153],\n",
       "       [154],\n",
       "       [155],\n",
       "       [156],\n",
       "       [157],\n",
       "       [158],\n",
       "       [159],\n",
       "       [160],\n",
       "       [161],\n",
       "       [162],\n",
       "       [163],\n",
       "       [164],\n",
       "       [165],\n",
       "       [166],\n",
       "       [167],\n",
       "       [168],\n",
       "       [169],\n",
       "       [170],\n",
       "       [171],\n",
       "       [172],\n",
       "       [173],\n",
       "       [174],\n",
       "       [175],\n",
       "       [176],\n",
       "       [177],\n",
       "       [178],\n",
       "       [179],\n",
       "       [180],\n",
       "       [181],\n",
       "       [182],\n",
       "       [183],\n",
       "       [184],\n",
       "       [185],\n",
       "       [186],\n",
       "       [187],\n",
       "       [188],\n",
       "       [189],\n",
       "       [190],\n",
       "       [191],\n",
       "       [192],\n",
       "       [193],\n",
       "       [194],\n",
       "       [195],\n",
       "       [196],\n",
       "       [197],\n",
       "       [198],\n",
       "       [199],\n",
       "       [200],\n",
       "       [201],\n",
       "       [202],\n",
       "       [203],\n",
       "       [204],\n",
       "       [205],\n",
       "       [206],\n",
       "       [207],\n",
       "       [208],\n",
       "       [209],\n",
       "       [210],\n",
       "       [211],\n",
       "       [212],\n",
       "       [213],\n",
       "       [214],\n",
       "       [215],\n",
       "       [216],\n",
       "       [217],\n",
       "       [218],\n",
       "       [219],\n",
       "       [220],\n",
       "       [221],\n",
       "       [222],\n",
       "       [223],\n",
       "       [224],\n",
       "       [225],\n",
       "       [226],\n",
       "       [227],\n",
       "       [228],\n",
       "       [229],\n",
       "       [230],\n",
       "       [231],\n",
       "       [232],\n",
       "       [233],\n",
       "       [234],\n",
       "       [235],\n",
       "       [236],\n",
       "       [237],\n",
       "       [238],\n",
       "       [239],\n",
       "       [240],\n",
       "       [241],\n",
       "       [242],\n",
       "       [243],\n",
       "       [244],\n",
       "       [245],\n",
       "       [246],\n",
       "       [247],\n",
       "       [248],\n",
       "       [249],\n",
       "       [250],\n",
       "       [251],\n",
       "       [252],\n",
       "       [253],\n",
       "       [254],\n",
       "       [255],\n",
       "       [256],\n",
       "       [257],\n",
       "       [258],\n",
       "       [259],\n",
       "       [260],\n",
       "       [261],\n",
       "       [262],\n",
       "       [263],\n",
       "       [264],\n",
       "       [265],\n",
       "       [266],\n",
       "       [267],\n",
       "       [268],\n",
       "       [269],\n",
       "       [270],\n",
       "       [271],\n",
       "       [272],\n",
       "       [273],\n",
       "       [274],\n",
       "       [275],\n",
       "       [276],\n",
       "       [277],\n",
       "       [278],\n",
       "       [279],\n",
       "       [280],\n",
       "       [281],\n",
       "       [282],\n",
       "       [283],\n",
       "       [284],\n",
       "       [285],\n",
       "       [286],\n",
       "       [287],\n",
       "       [288],\n",
       "       [289],\n",
       "       [290],\n",
       "       [291],\n",
       "       [292],\n",
       "       [293],\n",
       "       [294],\n",
       "       [295],\n",
       "       [296],\n",
       "       [297],\n",
       "       [298],\n",
       "       [299],\n",
       "       [300],\n",
       "       [301],\n",
       "       [302],\n",
       "       [303],\n",
       "       [304],\n",
       "       [305],\n",
       "       [306],\n",
       "       [307],\n",
       "       [308],\n",
       "       [309],\n",
       "       [310],\n",
       "       [311],\n",
       "       [312],\n",
       "       [313],\n",
       "       [314],\n",
       "       [315],\n",
       "       [316],\n",
       "       [317],\n",
       "       [318],\n",
       "       [319],\n",
       "       [320],\n",
       "       [321],\n",
       "       [322],\n",
       "       [323],\n",
       "       [324],\n",
       "       [325],\n",
       "       [326],\n",
       "       [327],\n",
       "       [328],\n",
       "       [329],\n",
       "       [330],\n",
       "       [331],\n",
       "       [332],\n",
       "       [333],\n",
       "       [334],\n",
       "       [335],\n",
       "       [336],\n",
       "       [337],\n",
       "       [338],\n",
       "       [339],\n",
       "       [340],\n",
       "       [341],\n",
       "       [342],\n",
       "       [343],\n",
       "       [344],\n",
       "       [345],\n",
       "       [346],\n",
       "       [347],\n",
       "       [348],\n",
       "       [349],\n",
       "       [350],\n",
       "       [351],\n",
       "       [352],\n",
       "       [353],\n",
       "       [354],\n",
       "       [355],\n",
       "       [356],\n",
       "       [357],\n",
       "       [358],\n",
       "       [359],\n",
       "       [360],\n",
       "       [361],\n",
       "       [362],\n",
       "       [363],\n",
       "       [364],\n",
       "       [365],\n",
       "       [366],\n",
       "       [367],\n",
       "       [368],\n",
       "       [369],\n",
       "       [370],\n",
       "       [371],\n",
       "       [372],\n",
       "       [373],\n",
       "       [374],\n",
       "       [375],\n",
       "       [376],\n",
       "       [377],\n",
       "       [378],\n",
       "       [379],\n",
       "       [380],\n",
       "       [381],\n",
       "       [382],\n",
       "       [383],\n",
       "       [384],\n",
       "       [385],\n",
       "       [386],\n",
       "       [387],\n",
       "       [388],\n",
       "       [389],\n",
       "       [390],\n",
       "       [391],\n",
       "       [392],\n",
       "       [393],\n",
       "       [394],\n",
       "       [395],\n",
       "       [396],\n",
       "       [397],\n",
       "       [398],\n",
       "       [399],\n",
       "       [400],\n",
       "       [401],\n",
       "       [402],\n",
       "       [403],\n",
       "       [404],\n",
       "       [405],\n",
       "       [406],\n",
       "       [407],\n",
       "       [408],\n",
       "       [409],\n",
       "       [410],\n",
       "       [411],\n",
       "       [412],\n",
       "       [413],\n",
       "       [414],\n",
       "       [415],\n",
       "       [416],\n",
       "       [417],\n",
       "       [418],\n",
       "       [419],\n",
       "       [420],\n",
       "       [421],\n",
       "       [422],\n",
       "       [423],\n",
       "       [424],\n",
       "       [425],\n",
       "       [426],\n",
       "       [427],\n",
       "       [428],\n",
       "       [429],\n",
       "       [430],\n",
       "       [431],\n",
       "       [432],\n",
       "       [433],\n",
       "       [434],\n",
       "       [435],\n",
       "       [436],\n",
       "       [437],\n",
       "       [438],\n",
       "       [439],\n",
       "       [440],\n",
       "       [441],\n",
       "       [442],\n",
       "       [443],\n",
       "       [444],\n",
       "       [445],\n",
       "       [446],\n",
       "       [447],\n",
       "       [448],\n",
       "       [449],\n",
       "       [450],\n",
       "       [451],\n",
       "       [452],\n",
       "       [453],\n",
       "       [454],\n",
       "       [455],\n",
       "       [456],\n",
       "       [457],\n",
       "       [458],\n",
       "       [459],\n",
       "       [460],\n",
       "       [461],\n",
       "       [462],\n",
       "       [463],\n",
       "       [464],\n",
       "       [465],\n",
       "       [466],\n",
       "       [467],\n",
       "       [468],\n",
       "       [469],\n",
       "       [470],\n",
       "       [471],\n",
       "       [472],\n",
       "       [473],\n",
       "       [474],\n",
       "       [475],\n",
       "       [476],\n",
       "       [477],\n",
       "       [478],\n",
       "       [479],\n",
       "       [480],\n",
       "       [481],\n",
       "       [482],\n",
       "       [483],\n",
       "       [484],\n",
       "       [485],\n",
       "       [486],\n",
       "       [487],\n",
       "       [488],\n",
       "       [489],\n",
       "       [490],\n",
       "       [491],\n",
       "       [492],\n",
       "       [493],\n",
       "       [494],\n",
       "       [495],\n",
       "       [496],\n",
       "       [497],\n",
       "       [498],\n",
       "       [499],\n",
       "       [500],\n",
       "       [501],\n",
       "       [502],\n",
       "       [503],\n",
       "       [504],\n",
       "       [505],\n",
       "       [506],\n",
       "       [507],\n",
       "       [508],\n",
       "       [509],\n",
       "       [510],\n",
       "       [511],\n",
       "       [512],\n",
       "       [513],\n",
       "       [514],\n",
       "       [515],\n",
       "       [516],\n",
       "       [517],\n",
       "       [518],\n",
       "       [519],\n",
       "       [520],\n",
       "       [521],\n",
       "       [522],\n",
       "       [523],\n",
       "       [524],\n",
       "       [525],\n",
       "       [526],\n",
       "       [527],\n",
       "       [528],\n",
       "       [529],\n",
       "       [530],\n",
       "       [531],\n",
       "       [532],\n",
       "       [533],\n",
       "       [534],\n",
       "       [535],\n",
       "       [536],\n",
       "       [537],\n",
       "       [538],\n",
       "       [539],\n",
       "       [540],\n",
       "       [541],\n",
       "       [542],\n",
       "       [543],\n",
       "       [544],\n",
       "       [545],\n",
       "       [546],\n",
       "       [547],\n",
       "       [548],\n",
       "       [549],\n",
       "       [550],\n",
       "       [551],\n",
       "       [552],\n",
       "       [553],\n",
       "       [554],\n",
       "       [555],\n",
       "       [556],\n",
       "       [557],\n",
       "       [558],\n",
       "       [559],\n",
       "       [560],\n",
       "       [561],\n",
       "       [562],\n",
       "       [563],\n",
       "       [564],\n",
       "       [565],\n",
       "       [566],\n",
       "       [567],\n",
       "       [568],\n",
       "       [569],\n",
       "       [570],\n",
       "       [571],\n",
       "       [572],\n",
       "       [573],\n",
       "       [574],\n",
       "       [575],\n",
       "       [576],\n",
       "       [577],\n",
       "       [578],\n",
       "       [579],\n",
       "       [580],\n",
       "       [581],\n",
       "       [582],\n",
       "       [583],\n",
       "       [584],\n",
       "       [585],\n",
       "       [586],\n",
       "       [587],\n",
       "       [588],\n",
       "       [589],\n",
       "       [590],\n",
       "       [591],\n",
       "       [592],\n",
       "       [593],\n",
       "       [594],\n",
       "       [595],\n",
       "       [596],\n",
       "       [597],\n",
       "       [598],\n",
       "       [599],\n",
       "       [600],\n",
       "       [601],\n",
       "       [602],\n",
       "       [603],\n",
       "       [604],\n",
       "       [605],\n",
       "       [606],\n",
       "       [607],\n",
       "       [608],\n",
       "       [609],\n",
       "       [610],\n",
       "       [611],\n",
       "       [612],\n",
       "       [613],\n",
       "       [614],\n",
       "       [615],\n",
       "       [616],\n",
       "       [617],\n",
       "       [618],\n",
       "       [619],\n",
       "       [620],\n",
       "       [621],\n",
       "       [622],\n",
       "       [623],\n",
       "       [624],\n",
       "       [625],\n",
       "       [626],\n",
       "       [627],\n",
       "       [628],\n",
       "       [629],\n",
       "       [630],\n",
       "       [631],\n",
       "       [632],\n",
       "       [633],\n",
       "       [634],\n",
       "       [635],\n",
       "       [636],\n",
       "       [637],\n",
       "       [638],\n",
       "       [639],\n",
       "       [640],\n",
       "       [641],\n",
       "       [642],\n",
       "       [643],\n",
       "       [644],\n",
       "       [645],\n",
       "       [646],\n",
       "       [647],\n",
       "       [648],\n",
       "       [649],\n",
       "       [650],\n",
       "       [651],\n",
       "       [652],\n",
       "       [653],\n",
       "       [654],\n",
       "       [655],\n",
       "       [656],\n",
       "       [657],\n",
       "       [658],\n",
       "       [659],\n",
       "       [660],\n",
       "       [661],\n",
       "       [662],\n",
       "       [663],\n",
       "       [664],\n",
       "       [665],\n",
       "       [666],\n",
       "       [667],\n",
       "       [668]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_days = 20\n",
    "forecast = np.array([i for i in range(len(dates)+pred_days)]).reshape(-1, 1)\n",
    "adjusted_dates = forecast[:-20]\n",
    "adjusted_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "start = '1/22/2020'\n",
    "start_date = datetime.datetime.strptime(start, '%m/%d/%Y')\n",
    "forecast_dates = []\n",
    "for i in range(len(forecast)):\n",
    "    forecast_dates.append((start_date + datetime.timedelta(days=i)).strftime('%m/%d/%Y'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Covid cases worldwide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/21/2021</td>\n",
       "      <td>3e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11/22/2021</td>\n",
       "      <td>3e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11/23/2021</td>\n",
       "      <td>3e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11/24/2021</td>\n",
       "      <td>3e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11/25/2021</td>\n",
       "      <td>3e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11/26/2021</td>\n",
       "      <td>3e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11/27/2021</td>\n",
       "      <td>3e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11/28/2021</td>\n",
       "      <td>3e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11/29/2021</td>\n",
       "      <td>3e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11/30/2021</td>\n",
       "      <td>3e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12/01/2021</td>\n",
       "      <td>3e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12/02/2021</td>\n",
       "      <td>3e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12/03/2021</td>\n",
       "      <td>4e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12/04/2021</td>\n",
       "      <td>4e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12/05/2021</td>\n",
       "      <td>4e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12/06/2021</td>\n",
       "      <td>4e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12/07/2021</td>\n",
       "      <td>4e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12/08/2021</td>\n",
       "      <td>4e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12/09/2021</td>\n",
       "      <td>4e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12/10/2021</td>\n",
       "      <td>4e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Covid cases worldwide\n",
       "1   11/21/2021                  3e+07\n",
       "2   11/22/2021                  3e+07\n",
       "3   11/23/2021                  3e+07\n",
       "4   11/24/2021                  3e+07\n",
       "5   11/25/2021                  3e+07\n",
       "6   11/26/2021                  3e+07\n",
       "7   11/27/2021                  3e+07\n",
       "8   11/28/2021                  3e+07\n",
       "9   11/29/2021                  3e+07\n",
       "10  11/30/2021                  3e+07\n",
       "11  12/01/2021                  3e+07\n",
       "12  12/02/2021                  3e+07\n",
       "13  12/03/2021                  4e+07\n",
       "14  12/04/2021                  4e+07\n",
       "15  12/05/2021                  4e+07\n",
       "16  12/06/2021                  4e+07\n",
       "17  12/07/2021                  4e+07\n",
       "18  12/08/2021                  4e+07\n",
       "19  12/09/2021                  4e+07\n",
       "20  12/10/2021                  4e+07"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_pred.reshape(1,-1)[0]\n",
    "index = []\n",
    "for i in range(1,21):\n",
    "    index.append(i)\n",
    "df = pd.DataFrame({'Date': forecast_dates[-20:], 'Covid cases worldwide': np.round(y_pred[-20:])},index)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ce43836c8d3287a06e199a1efd6974587c1d8c7cdf961feda0708cd09e2869c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
